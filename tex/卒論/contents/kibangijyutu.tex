\chapter{基盤技術}
\label{chap:kibangijyutu}


%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{機械学習}
\subsection{機械学習とは}
機械学習とは, データを分析するための手法の1つであり, 大量のデータをコンピュータが学習し, データに潜んでいるルールやパターンといったものを発見する手法である. コンピュータが自ら学習した成果を用いて未知のデータの予測や発見を可能としている. 
コンピュータに反復的に学習させることでデータの中にある規則性, 特徴を発見することができる. 

この技術は現在では, 生物学や自動運転, 金融工学などさまざまな分野で大きな影響を与えている. 

機械学習の学習方法には教師あり学習, 教師なし学習, 強化学習の3種類が存在する. 

\subsection{教師あり学習}
読み込んだデータから入力と出力の関係を学習させ, データ間の関係性を学習させる手法である. 学習データには事前に「正解」のラベルを付与される. 
入力された値と正解のラベルのセットを繰り返し学習させることで, 未知の入力された値に対して正解となるデータを予測し, 出力することが可能となっている. 

教師あり学習の具体例には需要予測や株価予測, 画像認識などが挙げられる. 

\subsection{教師なし学習}
教師なし学習とは教師あり学習とは異なり, 学習データに「正解」のラベルは付与せずに, データセットのパターンからデータの関係を認知させる学習手法である. 正解と不正解が明確でない問題の解決策として用いられる. 
与えられたデータを繰り返し学習することによりそのデータにどのようなパターンが存在するかをコンピュータ自身が見つけ出すことができ, 未知のデータに対する予測, 識別を可能とする. 

教師なし学習で行う代表的な例は「クラスタリング」と「次元の削減」である. クラスタリングは複数のデータをそのデータの特徴に応じて幾つかのグループに分けることである. 次元の削減とはデータの次元数を減らすことでデータの特徴を表す情報を抽出することである. 

\subsection{強化学習}
強化学習は, 環境と相互作用しながら報酬をもとに行動を学習する枠組みのことである. 強化学習には方策に従って行動を学習する主体である「エージェント」と状態と報酬をエージェントに返し, エージェントが行動を与える対象である「環境」の2つが存在する. 

強化学習の具体例には将棋や囲碁などのゲームAIやロボットの単純動作の獲得などが挙げられる. Google社のSlphaGoというAIが韓国の囲碁プロ棋士に勝ったことで大きな話題を呼んだ学習方法である. 

%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{自然言語処理}
自然言語処理(Natural Language Processing)とは, 人間が使用する言語(自然言語)をコンピュータが分析する技術である. 自然言語処理によって大量の自然言語によるテキストデータが持つ意味を解析, 処理することができる. 
自然言語には文脈によって解釈が変わるなどの言葉の曖昧性や意味の重複といったものが含まれている. 従ってコンピュータがそれを処理するのは難しいため課題とされている. 

自然言語処理の活用事例として質問への回答, 文の要約や翻訳, テキスト分類などが挙げられる. 


%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{BERT}
\subsection{BERTとは}
BERT(Bidirectional Encoder Representations from Transformers)\cite{bert}とは, GoogleのJacob Devlinらによって2018年秋に提案された言語表現モデルである. BERTは, ラベル付されていないテキストから深い双方向表現を事前学習するように設計されている. その結果, 質問応答や言語推論などの幅広いタスクのためのモデルを作成するために, 出力層を1つ追加するだけで微調整することができる. BERTは11の自然言語処理タスクにおいて, GLUEスコアやSQuAD v1.1による質問応答テストのF1スコアなどで向上が確認された\cite{bert}. 

\subsection{学習方法}
BERTは事前学習とファインチューニングの2つのステップからなり, どちらのステップにおいてもtransformerモデルを用いる. transformerとは深層学習のベースとなっているモデルである. RNNやCNNの並列処理ができないという欠点がある中で, transformerは再帰や畳み込みは一切行わず, Attentionのみを用いることで並列化を可能にした. 基本的な構成はmulti-head attention層, add\&nom層(残差結合\&layer normalization), position-wise FNN層となっている. 

\subsection{事前学習}
事前学習では, Masked Language ModelとNext Sentence Predictionの2つのタスクを解く. Masked Language Modelは入力されたトークンをランダムにマスクし, マスクされたトークンを他のトークンから予測するタスクである. Next Sentence Predictionではある文章に対して, その後に出現する文を並べたペアを正例, ランダムな文章を並べたペアを負例として識別する問題を解く. この問題を解くことにより, 2つの文章が隣り合っているかどうかを予測するよう学習する. 
この2つのタスクは自己教師あり学習である. 自己教師あり学習とは, ラベルが付与されていない大量のデータセットを用いて, プレテキストタスク(擬似的なラベルが自動生成された代わりのタスク)を解くための事前学習を行うための学習方法である. 自己教師あり学習により人の手作業によるラベル付を必要とせずに大量のデータで学習することが可能である. 

\subsection{ファインチューニング}
ファインチューニングでは, ラベル付きデータを用いて特定のタスクに特化するように学習させる. 3.1.1で述べた通り, 解きたいタスクに応じてtransformerの上に出力層を1つ追加する. そしてラベル付きデータを用いて出力層とtransformerのパラメータを更新する. 

%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{Chinese Whispers}
\subsection{Chinese Whispersとは}
Chinese Whispers(CW)\cite{chinese-whispers}とはグラフクラスタリングのランダム化アルゴリズムである. CWは, 事前にクラスタの数を指定せずに, クラスタの数を自ら選択することができるため, 異なるサイズの分類を扱うことができる. そのため, クラスタの数が事前にわからないNLP問題に適している. 

\subsection{グラフの構築}
まず, ノードとエッジからなる, 重み付き無向グラフを作成する. これがクラスタリングの対象となるグラフである. ノードはテキスト文書などのデータの要素を表し, エッジはノードの関連性を表す. 

\subsection{クラスタリングの開始}
クラスタリングの開始時点では, 各ノードはそれぞれ異なるクラスタに属するものとする. すなわち, 1つのノードに対して1つのクラスタが割り振られる. 

\subsection{グラフの反復処理とクラスタの更新}
ノードは少数の反復ステップによってグラフを処理し, 接続されたノードの情報を受け取り, 接続するノードの中で最も近いノードのクラスを継承する. これは現在のノードに対するエッジの重みが最大となるクラスタであるため, 類似したノードが同じクラスタにグループ化される. 最も強いクラスが複数ある場合はランダムで1つ選ばれる. 一方でどのエッジにも接続されていないノードはクラスタリングのプロセスから除外されるため, 一部のノードはクラスタリングされないことがある. 

\subsection{クラスタリングの収束}
このグラフの反復処理とクラスタの更新を繰り返す. これによりクラスタが再構築され, 新たなクラスタの構築が進行する. これをクラスタリングが収束するまで繰り返す. この結果, 各ノードは最終的なクラスタに所属する. 