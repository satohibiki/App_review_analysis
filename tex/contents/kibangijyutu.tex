\chapter{基盤技術}
\label{chap:kibangijyutu}


%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{概要}
キーフレーズの自動抽出で使用したモデルは自然言語処理タスクに特化したBERTである. 本研究では, BERTの事前学習済みモデルをファインチューニングすることで自動抽出モデルを生成している. 
モデルのファインチューニングには機械学習の一種である教師あり学習を用いている. 
そして, 自動抽出したキーフレーズのクラスタリングにはグラフクラスリング手法の一種であるChinese Whispersを使用している. 
次にそれぞれの技術に関して詳細を示す. 

%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{BERT}
\subsection{BERTとは}\label{aboutbert}
BERT (Bidirectional Encoder Representations from Transformers) \cite{bert}とは, GoogleのJacob Devlinらによって2018年秋に提案された言語表現モデルである. BERTは, ラベル付けされていないテキストから深い双方向表現を事前学習するように設計されている. その結果, 質問応答や言語推論などの幅広いタスクのためのモデルを作成するために, 出力層を1つ追加するだけで微調整することができる. 
BERTは11の自然言語処理タスクにおいて, GLUEスコアやSQuAD v1.1による質問応答テストのF1スコアなどで向上が確認された. \cite{bert}

\subsection{学習方法}
BERTは事前学習とファインチューニングの2つのステップからなり, どちらのステップにおいてもTransformerモデルを用いる. Transformerとは深層学習のベースとなっているモデルである. RNNやCNNには並列処理ができないという欠点があるが, Transformerは再帰や畳み込みは一切行わず, Attentionのみを用いることで並列化を可能にした. 基本的な構成はmulti-head attention層, add \& norm層 (残差結合 \& layer normalization) , position-wise FNN層となっている. 

\subsection{事前学習}
事前学習では, Masked Language ModelとNext Sentence Predictionの2つのタスクを解く. Masked Language Modelは入力されたトークンをランダムにマスクし, マスクされたトークンを他のトークンから予測するタスクである. Next Sentence Predictionではある文章に対して, その後に出現する文を並べたペアを正例, ランダムな文章を並べたペアを負例として識別する問題を解く. この問題を解くことにより, 2つの文章が隣り合っているかどうかを予測するよう学習する. 
この2つのタスクは自己教師あり学習である. 自己教師あり学習とは, ラベルが付与されていない大量のデータセットを用いて, プレテキストタスク (擬似的なラベルが自動生成された代わりのタスク) を解くための学習方法である. 自己教師あり学習により人の手作業によるラベル付けを必要とせずに大量のデータで学習することができる. 

\subsection{ファインチューニング}
ファインチューニングでは, ラベル付きデータを用いて特定のタスクに特化するように学習させる. \ref{aboutbert}項で述べた通り, 解きたいタスクに応じてTransformerの上に出力層を1つ追加する. そしてラベル付きデータを用いて出力層とTransformerのパラメータを更新する. 

%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{機械学習}
\subsection{機械学習とは}
機械学習とは, データを分析するための手法の1つであり, 大量のデータをコンピュータが学習し, データに潜んでいるルールやパターンを発見する手法である. コンピュータが自ら学習した成果を用いて未知のデータの予測や発見を可能としており, 反復的に学習させることでデータの中にある規則性, 特徴を発見することができる. 機械学習は現在, 生物学や自動運転, 金融工学などさまざまな分野で大きな影響を与えている. 

機械学習の学習方法には教師あり学習, 教師なし学習, 強化学習の3種類が存在する. モデルのファインチューニングにおいて必要となる教師あり学習を中心に, 各学習方法について説明する. 

\subsection{教師あり学習}
読み込んだデータから入力と出力の関係を学習させ, データ間の関係性を学習させる手法である. 学習データには事前に``正解''のラベルが付与される. 
入力された値と正解のラベルのセットを繰り返し学習させることで, 未知の入力された値に対して正解となるデータを予測し, 出力することが可能となっている. 
教師あり学習の具体例には需要予測や株価予測, 画像認識などが挙げられる. 

モデルのファインチューニングではこの教師あり学習を使用している. 元のレビューを入力, レビューから抽出するべきキーフレーズを出力として, 入力と出力の関係をモデルに学習させることにより自動抽出モデルを生成した.

\subsection{教師なし学習}
学習データに``正解''のラベルは付与せずに, データセットのパターンからデータの関係を認知させる学習手法である. 主に正解と不正解が明確でない問題の解決策として用いられる. 
% 与えられたデータを繰り返し学習することによりそのデータにどのようなパターンが存在するかをコンピュータ自身が見つけ出すことができる. 
% そのため, 未知のデータに対する予測, 識別を可能とする. 

% 教師なし学習で行う代表的な例は``クラスタリング''と``次元の削減''である. クラスタリングは複数のデータをそのデータの特徴に応じて幾つかのグループに分けることである. 次元の削減とはデータの次元数を減らすことでデータの特徴を表す情報を抽出することである. 

\subsection{強化学習}
環境と相互作用しながら報酬をもとに行動を学習する手法である. 強化学習には方策に従って行動を学習する主体である``エージェント''と状態と報酬をエージェントに返し, エージェントが行動を与える対象である``環境''の2つが存在する. 

% 強化学習の具体例には将棋や囲碁などのゲームAIやロボットの単純動作の獲得などが挙げられる. Google社のAlphaGoというAIが韓国の囲碁プロ棋士に勝ったことで大きな話題を呼んだ学習方法である. 

%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

% \section{自然言語処理}
% 自然言語処理 (Natural Language Processing) とは, 人間が使用する言語 (自然言語) をコンピュータが分析する技術である. 自然言語処理によって, 自然言語による大量のテキストデータが持つ意味を解析, 処理することができる. 
% 自然言語には言葉の曖昧性や意味の重複が含まれている. 例えば, 同じ単語でも文脈が異なると意味が異なることや同じ意味でも様々な表現があることがある. したがって, 意味解釈や文脈解析でコンピュータが人間の意図に反した処理をしてしまうことが課題とされている. 

% 自然言語処理の活用事例として質問への回答, 文の要約や翻訳, テキスト分類などが挙げられる. 


%ーーーーーーーーーーーーーーーーーーーーーーーーーーーー

\section{Chinese Whispers}
\subsection{Chinese Whispersとは}
Chinese Whispers (CW) \cite{chinese-whispers}とはグラフクラスタリングのランダム化アルゴリズムである. Chinese Whispersは, 事前にクラスタの数を指定しないため, 異なるサイズのクラスタリングを扱うことができる. そのため, クラスタの数が事前にわからないNLP問題に適している. 
Chinese Whispersは次に示す4つの手順でクラスタリングを行う. 

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item \textbf{グラフの構築}\\
  ノードとエッジからなる重み付き無向グラフを作成する. これがクラスタリングの対象となるグラフである. ノードはテキスト文書などのデータの要素を表し, エッジはノード間の関連性を表す. \\
  
  \item \textbf{クラスタリングの開始}\\
  クラスタリングの開始時点では, 各ノードはそれぞれ異なるクラスタに属するものとする. すなわち, 1つのノードに対して1つのクラスタが割り振られる. \\
  
  \item \textbf{グラフの反復処理とクラスタの更新}\\
  ノードは少数の反復ステップによってグラフを処理し, 接続されたノードの情報を受け取り, 接続するノードの中で最も近いノードのクラスタを継承する. これは現在のノードに対するエッジの重みが最大となるクラスタであるため, 類似したノードが同じクラスタにグループ化される. 
  最も強いクラスタが複数ある場合はランダムで1つ選ばれる. 一方でどのエッジにも接続されていないノードはクラスタリングのプロセスから除外されるため, 一部のノードはクラスタリングされないことがある. \\
  
  \item \textbf{クラスタリングの収束}\\
  グラフの反復処理とクラスタの更新を繰り返すことにより, クラスタが再構築され, 新たなクラスタの構築が進行する. このステップをクラスタリングが収束するまで繰り返した結果, 各ノードは最終的なクラスタに所属する. 
\end{enumerate}